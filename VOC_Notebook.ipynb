{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "9ZOypCTy03t4",
    "outputId": "63decebb-e54e-467f-f0bc-2afaacd7fe50"
   },
   "source": [
    "### Training on VOC Dataset without cropping bounding boxes (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VTN2GQyYRaqn",
    "outputId": "cd780cc5-0f49-4c4c-9a6e-9cd660ad765b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 16,\n",
      " 'callbacks': {'checkpoint_mode': 'min',\n",
      "               'checkpoint_monitor': 'val_loss',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': True,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': False,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_0',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 150,\n",
      " 'mode': 'train',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [256, 256],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_0\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-03 14:04:47.673960: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-03 14:04:47.678765: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz\n",
      "2020-01-03 14:04:47.678977: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d35180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 14:04:47.679006: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-03 14:04:47.697365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-03 14:04:47.820418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 14:04:47.821292: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d34fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 14:04:47.821343: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
      "2020-01-03 14:04:47.821649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 14:04:47.822166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-03 14:04:47.822259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-03 14:04:47.833521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-03 14:04:47.840076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-03 14:04:47.846725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-03 14:04:47.858485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-03 14:04:47.861996: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-03 14:04:47.875685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-03 14:04:47.875867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 14:04:47.876401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 14:04:47.876783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-03 14:04:50.760232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-03 14:04:50.760306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-03 14:04:50.760325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-03 14:04:50.760643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 14:04:50.761140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 14:04:50.761486: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-03 14:04:50.761530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6726 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/150\n",
      "110/110 [==============================] - 101s 917ms/step - loss: 2.7452 - acc: 0.1102 - val_loss: 2.2892 - val_acc: 0.1770\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.28924, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-01-0.18.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - 89s 810ms/step - loss: 2.4833 - acc: 0.1466 - val_loss: 2.1610 - val_acc: 0.2446\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.28924 to 2.16099, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-02-0.24.h5\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - 89s 810ms/step - loss: 2.3331 - acc: 0.1784 - val_loss: 2.0577 - val_acc: 0.2990\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.16099 to 2.05766, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-03-0.30.h5\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - 90s 818ms/step - loss: 2.2481 - acc: 0.2170 - val_loss: 1.9616 - val_acc: 0.3603\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.05766 to 1.96163, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-04-0.36.h5\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - 89s 812ms/step - loss: 2.1006 - acc: 0.2756 - val_loss: 1.8610 - val_acc: 0.4021\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.96163 to 1.86105, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-05-0.40.h5\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - 90s 817ms/step - loss: 2.0300 - acc: 0.3080 - val_loss: 1.7616 - val_acc: 0.4284\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.86105 to 1.76162, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-06-0.43.h5\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - 89s 812ms/step - loss: 1.8738 - acc: 0.3773 - val_loss: 1.6769 - val_acc: 0.4651\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.76162 to 1.67691, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-07-0.47.h5\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - 90s 815ms/step - loss: 1.7940 - acc: 0.4023 - val_loss: 1.6013 - val_acc: 0.4857\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.67691 to 1.60126, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-08-0.49.h5\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - 90s 817ms/step - loss: 1.7142 - acc: 0.4301 - val_loss: 1.5426 - val_acc: 0.5029\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.60126 to 1.54262, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-09-0.50.h5\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - 90s 815ms/step - loss: 1.6367 - acc: 0.4494 - val_loss: 1.4987 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.54262 to 1.49869, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-10-0.53.h5\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - 89s 813ms/step - loss: 1.5192 - acc: 0.5000 - val_loss: 1.4721 - val_acc: 0.5361\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.49869 to 1.47210, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-11-0.54.h5\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - 90s 817ms/step - loss: 1.4191 - acc: 0.5330 - val_loss: 1.4537 - val_acc: 0.5372\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.47210 to 1.45368, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-12-0.54.h5\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - 90s 814ms/step - loss: 1.3860 - acc: 0.5506 - val_loss: 1.4264 - val_acc: 0.5475\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.45368 to 1.42637, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-13-0.55.h5\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - 90s 819ms/step - loss: 1.3035 - acc: 0.5778 - val_loss: 1.4045 - val_acc: 0.5533\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.42637 to 1.40447, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-14-0.55.h5\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - 90s 821ms/step - loss: 1.2632 - acc: 0.5949 - val_loss: 1.3924 - val_acc: 0.5624\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.40447 to 1.39235, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-15-0.56.h5\n",
      "Epoch 16/150\n",
      "110/110 [==============================] - 90s 819ms/step - loss: 1.1479 - acc: 0.6273 - val_loss: 1.3781 - val_acc: 0.5682\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.39235 to 1.37805, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-16-0.57.h5\n",
      "Epoch 17/150\n",
      "110/110 [==============================] - 90s 817ms/step - loss: 1.0909 - acc: 0.6608 - val_loss: 1.3710 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.37805 to 1.37096, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-17-0.58.h5\n",
      "Epoch 18/150\n",
      "110/110 [==============================] - 89s 813ms/step - loss: 1.0181 - acc: 0.6773 - val_loss: 1.3780 - val_acc: 0.5722\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.37096\n",
      "Epoch 19/150\n",
      "110/110 [==============================] - 89s 813ms/step - loss: 0.9502 - acc: 0.7045 - val_loss: 1.4145 - val_acc: 0.5727\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.37096\n",
      "Epoch 20/150\n",
      "110/110 [==============================] - 89s 807ms/step - loss: 0.8989 - acc: 0.7119 - val_loss: 1.4093 - val_acc: 0.5802\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.37096\n",
      "Epoch 21/150\n",
      "110/110 [==============================] - 89s 813ms/step - loss: 0.8760 - acc: 0.7409 - val_loss: 1.4007 - val_acc: 0.5819\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.37096\n",
      "Epoch 22/150\n",
      "110/110 [==============================] - 89s 808ms/step - loss: 0.8221 - acc: 0.7324 - val_loss: 1.4248 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.37096\n",
      "Epoch 23/150\n",
      "110/110 [==============================] - 89s 810ms/step - loss: 0.7255 - acc: 0.7739 - val_loss: 1.4533 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.37096\n",
      "Epoch 24/150\n",
      "110/110 [==============================] - 90s 816ms/step - loss: 0.6919 - acc: 0.7812 - val_loss: 1.5079 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.37096\n",
      "Epoch 25/150\n",
      "110/110 [==============================] - 89s 806ms/step - loss: 0.6254 - acc: 0.8057 - val_loss: 1.4688 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.37096\n",
      "Epoch 26/150\n",
      "110/110 [==============================] - 86s 783ms/step - loss: 0.5853 - acc: 0.8267 - val_loss: 1.5164 - val_acc: 0.5722\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.37096\n",
      "Epoch 27/150\n",
      "110/110 [==============================] - 87s 792ms/step - loss: 0.5724 - acc: 0.8256 - val_loss: 1.5646 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.37096\n",
      "Epoch 28/150\n",
      "110/110 [==============================] - 89s 811ms/step - loss: 0.5178 - acc: 0.8341 - val_loss: 1.5557 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.37096\n",
      "Epoch 29/150\n",
      "110/110 [==============================] - 88s 797ms/step - loss: 0.4782 - acc: 0.8597 - val_loss: 1.6141 - val_acc: 0.5802\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.37096\n",
      "Epoch 30/150\n",
      "110/110 [==============================] - 88s 796ms/step - loss: 0.4407 - acc: 0.8585 - val_loss: 1.6219 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.37096\n",
      "Epoch 31/150\n",
      "110/110 [==============================] - 87s 789ms/step - loss: 0.4000 - acc: 0.8750 - val_loss: 1.6327 - val_acc: 0.5802\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.37096\n",
      "Epoch 32/150\n",
      "110/110 [==============================] - 87s 794ms/step - loss: 0.3787 - acc: 0.8835 - val_loss: 1.6360 - val_acc: 0.5819\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.37096\n",
      "Epoch 33/150\n",
      "110/110 [==============================] - 87s 791ms/step - loss: 0.3528 - acc: 0.8881 - val_loss: 1.7442 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.37096\n",
      "Epoch 34/150\n",
      "110/110 [==============================] - 88s 797ms/step - loss: 0.3242 - acc: 0.9011 - val_loss: 1.7714 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.37096\n",
      "Epoch 35/150\n",
      "110/110 [==============================] - 87s 792ms/step - loss: 0.2894 - acc: 0.9108 - val_loss: 1.7744 - val_acc: 0.5819\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.37096\n",
      "Epoch 36/150\n",
      "110/110 [==============================] - 89s 805ms/step - loss: 0.3040 - acc: 0.9091 - val_loss: 1.7692 - val_acc: 0.5733\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.37096\n",
      "Epoch 37/150\n",
      "110/110 [==============================] - 88s 797ms/step - loss: 0.2666 - acc: 0.9136 - val_loss: 1.8484 - val_acc: 0.5739\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.37096\n",
      "Epoch 38/150\n",
      "110/110 [==============================] - 88s 796ms/step - loss: 0.2158 - acc: 0.9364 - val_loss: 1.8946 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.37096\n",
      "Epoch 39/150\n",
      "110/110 [==============================] - 88s 803ms/step - loss: 0.2109 - acc: 0.9398 - val_loss: 1.8686 - val_acc: 0.5819\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.37096\n",
      "Epoch 40/150\n",
      "110/110 [==============================] - 87s 793ms/step - loss: 0.1921 - acc: 0.9455 - val_loss: 1.9745 - val_acc: 0.5739\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.37096\n",
      "Epoch 41/150\n",
      "110/110 [==============================] - 87s 789ms/step - loss: 0.2051 - acc: 0.9364 - val_loss: 1.9566 - val_acc: 0.5699\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.37096\n",
      "Epoch 42/150\n",
      "110/110 [==============================] - 88s 800ms/step - loss: 0.1589 - acc: 0.9551 - val_loss: 2.0150 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.37096\n",
      "Epoch 43/150\n",
      "110/110 [==============================] - 87s 791ms/step - loss: 0.1617 - acc: 0.9528 - val_loss: 1.9786 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.37096\n",
      "Epoch 44/150\n",
      "110/110 [==============================] - 87s 790ms/step - loss: 0.1354 - acc: 0.9636 - val_loss: 2.0242 - val_acc: 0.5699\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.37096\n",
      "Epoch 45/150\n",
      "110/110 [==============================] - 87s 794ms/step - loss: 0.1391 - acc: 0.9557 - val_loss: 2.0824 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.37096\n",
      "Epoch 46/150\n",
      "110/110 [==============================] - 87s 795ms/step - loss: 0.1233 - acc: 0.9693 - val_loss: 2.0635 - val_acc: 0.5790\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.37096\n",
      "Epoch 47/150\n",
      "110/110 [==============================] - 88s 800ms/step - loss: 0.1298 - acc: 0.9699 - val_loss: 2.1504 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.37096\n",
      "Epoch 48/150\n",
      "110/110 [==============================] - 87s 793ms/step - loss: 0.1263 - acc: 0.9699 - val_loss: 2.1478 - val_acc: 0.5710\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.37096\n",
      "Epoch 49/150\n",
      "110/110 [==============================] - 87s 793ms/step - loss: 0.0957 - acc: 0.9761 - val_loss: 2.1589 - val_acc: 0.5710\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.37096\n",
      "Epoch 50/150\n",
      "110/110 [==============================] - 87s 790ms/step - loss: 0.0882 - acc: 0.9818 - val_loss: 2.2355 - val_acc: 0.5733\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.37096\n",
      "Epoch 51/150\n",
      "106/110 [===========================>..] - ETA: 1s - loss: 0.0816 - acc: 0.9770[INFO]: You have entered CTRL+C.. Wait to finalize\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPa38zQrp8zH"
   },
   "source": [
    "Testing with best epoch RESNET50_0-17-0.58.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a4O4cc40opGY",
    "outputId": "08182b3c-5941-4608-ca65-e92b7a3db014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 16,\n",
      " 'best_file': 'RESNET50_0-17-0.58.h5',\n",
      " 'callbacks': {'checkpoint_mode': 'min',\n",
      "               'checkpoint_monitor': 'val_loss',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': True,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': False,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_0',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 150,\n",
      " 'mode': 'test',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [256, 256],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_0\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-05 21:59:41.964337: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-05 21:59:41.968804: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000179999 Hz\n",
      "2020-01-05 21:59:41.969028: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x39ff800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-05 21:59:41.969055: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-05 21:59:41.970856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-05 21:59:42.081364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 21:59:42.082073: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x39ff640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-05 21:59:42.082108: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-01-05 21:59:42.082313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 21:59:42.082883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-05 21:59:42.082947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-05 21:59:42.084416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-05 21:59:42.085957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-05 21:59:42.086363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-05 21:59:42.087721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-05 21:59:42.088425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-05 21:59:42.091341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-05 21:59:42.091454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 21:59:42.092018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 21:59:42.092602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-05 21:59:45.100027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-05 21:59:45.100098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-05 21:59:45.100111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-05 21:59:45.100406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 21:59:45.101009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 21:59:45.101594: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-05 21:59:45.101635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "1746/1746 [==============================] - 752s 430ms/step\n",
      "[1.3709624145845922, 0.5778923253150057]\n",
      "1746/1746 [==============================] - 28s 16ms/step\n",
      "[[113  22  18   1   2   4   2   2   9   1   1]\n",
      " [ 17 132  26  20  38  17  11   8  17   9   3]\n",
      " [  2  26 112   3  26   4   1   0  14   5   3]\n",
      " [  0  21   1 108   5  16   1   0   2   4   0]\n",
      " [  3  39   8   4 107   0  16   0   3   8   1]\n",
      " [  4  14   0   2   1  85   2   6   0   1   0]\n",
      " [  2  15   2   0   5   1 111   1  17   0   2]\n",
      " [ 10  14   1   0   1   7   0  29   5  18   4]\n",
      " [  7  30   4   2   8   6   2   3  85   2   4]\n",
      " [  0   9   0   0  12   0   0   0   3 116   0]\n",
      " [  8  30   1   0   2   5   4   6  10   0  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.66       175\n",
      "           1       0.38      0.44      0.41       298\n",
      "           2       0.65      0.57      0.61       196\n",
      "           3       0.77      0.68      0.72       158\n",
      "           4       0.52      0.57      0.54       189\n",
      "           5       0.59      0.74      0.65       115\n",
      "           6       0.74      0.71      0.73       156\n",
      "           7       0.53      0.33      0.40        89\n",
      "           8       0.52      0.56      0.53       153\n",
      "           9       0.71      0.83      0.76       140\n",
      "          10       0.38      0.14      0.21        77\n",
      "\n",
      "    accuracy                           0.58      1746\n",
      "   macro avg       0.59      0.56      0.57      1746\n",
      "weighted avg       0.58      0.58      0.57      1746\n",
      "\n",
      "1009\n",
      "0.5778923253150057\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_0_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yBz3nk6OrNv"
   },
   "source": [
    "### Training with cropped images with 128x128 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M6i7hCL1VLfr",
    "outputId": "0ea49a72-1fb7-4025-d718-a3babf73df98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 128,\n",
      " 'callbacks': {'checkpoint_mode': 'min',\n",
      "               'checkpoint_monitor': 'val_loss',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': True,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': True,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_0',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 200,\n",
      " 'mode': 'train',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 12,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [128, 128],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_0\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-02 12:53:38.985306: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-02 12:53:38.990483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\n",
      "2020-01-02 12:53:38.990715: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3be4fc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-02 12:53:38.990746: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-02 12:53:38.992654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-02 12:53:39.101547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 12:53:39.102212: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3be4e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-02 12:53:39.102237: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-01-02 12:53:39.102410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 12:53:39.102907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-02 12:53:39.102962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-02 12:53:39.104605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-02 12:53:39.106134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-02 12:53:39.106513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-02 12:53:39.108093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-02 12:53:39.108865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-02 12:53:39.112180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-02 12:53:39.112290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 12:53:39.112895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 12:53:39.113393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-02 12:53:41.919986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-02 12:53:41.920048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-02 12:53:41.920059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-02 12:53:41.920303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 12:53:41.920910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 12:53:41.921388: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-02 12:53:41.921430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/200\n",
      "14/14 [==============================] - 65s 5s/step - loss: 2.8504 - acc: 0.1429 - val_loss: 2.5487 - val_acc: 0.1168\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.54872, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-01-2.55.hdf5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 42s 3s/step - loss: 2.7229 - acc: 0.1518 - val_loss: 2.3995 - val_acc: 0.1094\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.54872 to 2.39945, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-02-2.40.hdf5\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.7152 - acc: 0.1473 - val_loss: 2.4080 - val_acc: 0.1546\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.39945\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.6470 - acc: 0.1663 - val_loss: 2.3331 - val_acc: 0.1896\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.39945 to 2.33311, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-04-2.33.hdf5\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.6028 - acc: 0.1613 - val_loss: 2.3135 - val_acc: 0.1764\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.33311 to 2.31348, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-05-2.31.hdf5\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.5928 - acc: 0.1752 - val_loss: 2.1782 - val_acc: 0.1649\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.31348 to 2.17823, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-06-2.18.hdf5\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.5468 - acc: 0.1557 - val_loss: 2.2570 - val_acc: 0.1964\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.17823\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.5357 - acc: 0.1758 - val_loss: 2.1347 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.17823 to 2.13470, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-08-2.13.hdf5\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.5351 - acc: 0.1747 - val_loss: 2.2250 - val_acc: 0.2016\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.13470\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.4693 - acc: 0.1763 - val_loss: 2.0959 - val_acc: 0.2257\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.13470 to 2.09587, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-10-2.10.hdf5\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.3571 - acc: 0.1998 - val_loss: 2.1491 - val_acc: 0.2606\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.09587\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.2662 - acc: 0.2394 - val_loss: 2.1846 - val_acc: 0.1993\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.09587\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.1945 - acc: 0.2528 - val_loss: 2.1628 - val_acc: 0.2732\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.09587\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.1398 - acc: 0.2511 - val_loss: 1.9804 - val_acc: 0.3110\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.09587 to 1.98043, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-14-1.98.hdf5\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.1522 - acc: 0.2589 - val_loss: 2.0983 - val_acc: 0.3013\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.98043\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.0672 - acc: 0.2807 - val_loss: 2.0768 - val_acc: 0.2955\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.98043\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.0121 - acc: 0.3136 - val_loss: 2.1061 - val_acc: 0.2904\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.98043\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.1078 - acc: 0.2706 - val_loss: 2.0487 - val_acc: 0.2847\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.98043\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.0518 - acc: 0.2857 - val_loss: 2.0462 - val_acc: 0.3242\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.98043\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.0462 - acc: 0.2941 - val_loss: 1.9927 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.98043\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 2.0275 - acc: 0.2980 - val_loss: 1.9238 - val_acc: 0.3568\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.98043 to 1.92377, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-21-1.92.hdf5\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.9078 - acc: 0.3371 - val_loss: 1.9925 - val_acc: 0.2950\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.92377\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.9443 - acc: 0.3454 - val_loss: 1.9263 - val_acc: 0.3419\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.92377\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.8245 - acc: 0.3800 - val_loss: 1.9557 - val_acc: 0.2990\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.92377\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.8323 - acc: 0.3728 - val_loss: 2.0383 - val_acc: 0.3116\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.92377\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.7647 - acc: 0.4035 - val_loss: 1.8211 - val_acc: 0.3946\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.92377 to 1.82105, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-26-1.82.hdf5\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.7652 - acc: 0.3817 - val_loss: 1.9161 - val_acc: 0.3299\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.82105\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.6922 - acc: 0.4152 - val_loss: 1.9352 - val_acc: 0.3597\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.82105\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.7017 - acc: 0.4325 - val_loss: 1.9052 - val_acc: 0.3459\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.82105\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.6977 - acc: 0.4213 - val_loss: 1.8504 - val_acc: 0.3849\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.82105\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.6928 - acc: 0.4269 - val_loss: 1.7127 - val_acc: 0.4656\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.82105 to 1.71267, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-31-1.71.hdf5\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.6362 - acc: 0.4498 - val_loss: 1.7352 - val_acc: 0.4296\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.71267\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.6815 - acc: 0.4314 - val_loss: 1.7887 - val_acc: 0.3872\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.71267\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.5799 - acc: 0.4565 - val_loss: 1.7606 - val_acc: 0.4158\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.71267\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.5749 - acc: 0.4727 - val_loss: 1.7692 - val_acc: 0.4170\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.71267\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.5624 - acc: 0.4833 - val_loss: 1.7711 - val_acc: 0.4490\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.71267\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4693 - acc: 0.5296 - val_loss: 1.7855 - val_acc: 0.3769\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.71267\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4363 - acc: 0.5206 - val_loss: 1.8212 - val_acc: 0.3688\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.71267\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4017 - acc: 0.5262 - val_loss: 1.6981 - val_acc: 0.4588\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.71267 to 1.69811, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-39-1.70.hdf5\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4120 - acc: 0.5246 - val_loss: 1.7255 - val_acc: 0.4227\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.69811\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.3891 - acc: 0.5318 - val_loss: 1.6157 - val_acc: 0.4885\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.69811 to 1.61572, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-41-1.62.hdf5\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.3679 - acc: 0.5379 - val_loss: 1.7588 - val_acc: 0.4530\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.61572\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.3364 - acc: 0.5619 - val_loss: 1.5110 - val_acc: 0.4863\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.61572 to 1.51098, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-43-1.51.hdf5\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 1.3773 - acc: 0.5374 - val_loss: 1.7652 - val_acc: 0.3981\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.51098\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.3834 - acc: 0.5474 - val_loss: 1.7374 - val_acc: 0.4187\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.51098\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.3588 - acc: 0.5396 - val_loss: 1.7432 - val_acc: 0.4078\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.51098\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.3348 - acc: 0.5541 - val_loss: 1.6068 - val_acc: 0.4359\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.51098\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.2540 - acc: 0.6071 - val_loss: 1.6763 - val_acc: 0.4233\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.51098\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.1445 - acc: 0.6429 - val_loss: 1.6496 - val_acc: 0.4737\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.51098\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.1162 - acc: 0.6317 - val_loss: 1.7421 - val_acc: 0.4496\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.51098\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.0932 - acc: 0.6417 - val_loss: 1.7015 - val_acc: 0.4616\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.51098\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.0698 - acc: 0.6546 - val_loss: 1.6162 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.51098\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.1492 - acc: 0.6250 - val_loss: 1.5183 - val_acc: 0.4851\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.51098\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.1053 - acc: 0.6473 - val_loss: 1.5384 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.51098\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 1.0917 - acc: 0.6412 - val_loss: 1.9005 - val_acc: 0.4158\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.51098\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.0874 - acc: 0.6244 - val_loss: 1.6617 - val_acc: 0.4353\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.51098\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.0842 - acc: 0.6317 - val_loss: 1.5557 - val_acc: 0.4404\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.51098\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.0589 - acc: 0.6674 - val_loss: 1.7150 - val_acc: 0.4662\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.51098\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.0319 - acc: 0.6657 - val_loss: 1.6976 - val_acc: 0.4336\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.51098\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.9839 - acc: 0.6942 - val_loss: 1.6108 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.51098\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.9155 - acc: 0.7238 - val_loss: 1.7222 - val_acc: 0.4737\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.51098\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8734 - acc: 0.7321 - val_loss: 1.8338 - val_acc: 0.4840\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.51098\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8579 - acc: 0.7305 - val_loss: 1.5771 - val_acc: 0.4794\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.51098\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8857 - acc: 0.7221 - val_loss: 1.4599 - val_acc: 0.5258\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.51098 to 1.45992, saving model to experiments/RESNET50_0/checkpoints/RESNET50_0-64-1.46.hdf5\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8143 - acc: 0.7444 - val_loss: 1.5880 - val_acc: 0.4742\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.45992\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8604 - acc: 0.7121 - val_loss: 1.7024 - val_acc: 0.4851\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.45992\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8160 - acc: 0.7478 - val_loss: 1.5162 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.45992\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.7824 - acc: 0.7589 - val_loss: 1.6467 - val_acc: 0.4885\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.45992\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.7258 - acc: 0.7768 - val_loss: 1.6430 - val_acc: 0.4788\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.45992\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.8710 - acc: 0.7176 - val_loss: 1.7782 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.45992\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.7260 - acc: 0.7667 - val_loss: 1.6241 - val_acc: 0.4931\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.45992\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.9123 - acc: 0.7054 - val_loss: 1.8869 - val_acc: 0.4628\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.45992\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.7175 - acc: 0.7762 - val_loss: 1.6209 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.45992\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6056 - acc: 0.8164 - val_loss: 1.5958 - val_acc: 0.5166\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.45992\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6581 - acc: 0.7946 - val_loss: 1.7949 - val_acc: 0.4513\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.45992\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6077 - acc: 0.8092 - val_loss: 1.6621 - val_acc: 0.4588\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.45992\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6339 - acc: 0.8047 - val_loss: 1.6858 - val_acc: 0.4685\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.45992\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.5833 - acc: 0.8237 - val_loss: 1.5157 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.45992\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.5611 - acc: 0.8158 - val_loss: 1.6507 - val_acc: 0.5624\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.45992\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.5667 - acc: 0.8198 - val_loss: 1.6442 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.45992\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.5596 - acc: 0.8320 - val_loss: 1.7970 - val_acc: 0.4885\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.45992\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6109 - acc: 0.8153 - val_loss: 1.8368 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.45992\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.5907 - acc: 0.8164 - val_loss: 1.7394 - val_acc: 0.4926\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.45992\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.5179 - acc: 0.8594 - val_loss: 1.9484 - val_acc: 0.4645\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.45992\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.5302 - acc: 0.8331 - val_loss: 1.8607 - val_acc: 0.4943\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.45992\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4769 - acc: 0.8583 - val_loss: 1.7183 - val_acc: 0.5143\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.45992\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4540 - acc: 0.8672 - val_loss: 1.5549 - val_acc: 0.5103\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.45992\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4098 - acc: 0.8811 - val_loss: 1.8199 - val_acc: 0.4696\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.45992\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4064 - acc: 0.8744 - val_loss: 1.8128 - val_acc: 0.4937\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.45992\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4099 - acc: 0.8756 - val_loss: 1.7511 - val_acc: 0.5029\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.45992\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3968 - acc: 0.8895 - val_loss: 1.9075 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.45992\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3791 - acc: 0.8834 - val_loss: 1.9913 - val_acc: 0.4221\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.45992\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3660 - acc: 0.8956 - val_loss: 1.6233 - val_acc: 0.5349\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.45992\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.5097 - acc: 0.8471 - val_loss: 1.7704 - val_acc: 0.5487\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.45992\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.4488 - acc: 0.8627 - val_loss: 1.9848 - val_acc: 0.4989\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.45992\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.4904 - acc: 0.8577 - val_loss: 1.6403 - val_acc: 0.5057\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.45992\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3655 - acc: 0.9079 - val_loss: 1.8869 - val_acc: 0.4691\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.45992\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2797 - acc: 0.9291 - val_loss: 1.8667 - val_acc: 0.4874\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.45992\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3496 - acc: 0.8990 - val_loss: 1.8524 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.45992\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3323 - acc: 0.9035 - val_loss: 2.0802 - val_acc: 0.5057\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.45992\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3350 - acc: 0.9018 - val_loss: 2.0741 - val_acc: 0.4565\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.45992\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2981 - acc: 0.9129 - val_loss: 1.7549 - val_acc: 0.5132\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.45992\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3010 - acc: 0.9180 - val_loss: 1.8503 - val_acc: 0.5275\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.45992\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2890 - acc: 0.9219 - val_loss: 1.8057 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.45992\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2666 - acc: 0.9213 - val_loss: 2.1718 - val_acc: 0.5034\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.45992\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.3365 - acc: 0.9018 - val_loss: 2.1206 - val_acc: 0.5103\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.45992\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2785 - acc: 0.9180 - val_loss: 1.7842 - val_acc: 0.5183\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.45992\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.3003 - acc: 0.9219 - val_loss: 1.7684 - val_acc: 0.5470\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.45992\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2382 - acc: 0.9353 - val_loss: 2.2704 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.45992\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2312 - acc: 0.9431 - val_loss: 2.0449 - val_acc: 0.5338\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.45992\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2296 - acc: 0.9375 - val_loss: 2.2098 - val_acc: 0.4393\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.45992\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1851 - acc: 0.9598 - val_loss: 2.3310 - val_acc: 0.4582\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.45992\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1865 - acc: 0.9492 - val_loss: 1.8575 - val_acc: 0.5263\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.45992\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1853 - acc: 0.9487 - val_loss: 2.1643 - val_acc: 0.4444\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.45992\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1885 - acc: 0.9537 - val_loss: 1.8458 - val_acc: 0.5246\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.45992\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2001 - acc: 0.9475 - val_loss: 1.8977 - val_acc: 0.5407\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.45992\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2051 - acc: 0.9431 - val_loss: 2.1473 - val_acc: 0.4593\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.45992\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2396 - acc: 0.9297 - val_loss: 2.2410 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.45992\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1895 - acc: 0.9464 - val_loss: 1.8086 - val_acc: 0.5716\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.45992\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.2385 - acc: 0.9358 - val_loss: 1.8729 - val_acc: 0.5304\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.45992\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1967 - acc: 0.9475 - val_loss: 2.3013 - val_acc: 0.4685\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.45992\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1540 - acc: 0.9548 - val_loss: 2.3328 - val_acc: 0.4530\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.45992\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1484 - acc: 0.9643 - val_loss: 1.7828 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.45992\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1656 - acc: 0.9492 - val_loss: 2.1129 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.45992\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1596 - acc: 0.9537 - val_loss: 2.2565 - val_acc: 0.4645\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.45992\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1367 - acc: 0.9676 - val_loss: 2.4210 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.45992\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1426 - acc: 0.9676 - val_loss: 2.1603 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.45992\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1260 - acc: 0.9732 - val_loss: 2.0206 - val_acc: 0.5206\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.45992\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1358 - acc: 0.9660 - val_loss: 1.8511 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.45992\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1843 - acc: 0.9481 - val_loss: 2.3443 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.45992\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1288 - acc: 0.9660 - val_loss: 2.4811 - val_acc: 0.4502\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.45992\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1212 - acc: 0.9704 - val_loss: 1.9008 - val_acc: 0.5304\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.45992\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.1068 - acc: 0.9816 - val_loss: 2.3787 - val_acc: 0.4977\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.45992\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.0973 - acc: 0.9771 - val_loss: 2.1901 - val_acc: 0.5023\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.45992\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.1113 - acc: 0.9727 - val_loss: 2.3712 - val_acc: 0.4754\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.45992\n",
      "Epoch 136/200\n",
      "13/14 [==========================>...] - ETA: 1s - loss: 0.1016 - acc: 0.9760Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-3:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-2:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-22:\n",
      "[INFO]: You have entered CTRL+C.. Wait to finalize\n",
      "Process ForkPoolWorker-8:\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J80bgR6krkd-"
   },
   "source": [
    "Stop Based on Val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mG_2DEcnSKDY",
    "outputId": "ddcff49f-bcff-4d5d-f3e3-cc85e99f02a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 16,\n",
      " 'callbacks': {'checkpoint_mode': 'max',\n",
      "               'checkpoint_monitor': 'val_acc',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': False,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': True,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_1',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 200,\n",
      " 'mode': 'train',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [128, 128],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_1\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-02 23:37:22.049424: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-02 23:37:22.054233: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000160000 Hz\n",
      "2020-01-02 23:37:22.054432: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32cafc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-02 23:37:22.054460: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-02 23:37:22.056268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-02 23:37:22.166801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 23:37:22.167683: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32cae00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-02 23:37:22.167721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-01-02 23:37:22.167892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 23:37:22.168608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-02 23:37:22.168702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-02 23:37:22.170467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-02 23:37:22.172197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-02 23:37:22.172698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-02 23:37:22.174327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-02 23:37:22.175226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-02 23:37:22.178449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-02 23:37:22.178679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 23:37:22.179822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 23:37:22.180414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-02 23:37:25.120430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-02 23:37:25.120486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-02 23:37:25.120499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-02 23:37:25.120738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 23:37:25.121444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-02 23:37:25.121949: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-02 23:37:25.121999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/200\n",
      "110/110 [==============================] - 62s 568ms/step - loss: 3.0511 - acc: 0.1040 - val_loss: 2.3609 - val_acc: 0.1426\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.14261, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-01-0.14.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/200\n",
      "110/110 [==============================] - 52s 477ms/step - loss: 2.7837 - acc: 0.1364 - val_loss: 2.2598 - val_acc: 0.1913\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.14261 to 0.19129, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-02-0.19.h5\n",
      "Epoch 3/200\n",
      "110/110 [==============================] - 52s 472ms/step - loss: 2.6385 - acc: 0.1466 - val_loss: 2.1894 - val_acc: 0.2194\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.19129 to 0.21936, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-03-0.22.h5\n",
      "Epoch 4/200\n",
      "110/110 [==============================] - 52s 470ms/step - loss: 2.5132 - acc: 0.1716 - val_loss: 2.1314 - val_acc: 0.2532\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.21936 to 0.25315, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-04-0.25.h5\n",
      "Epoch 5/200\n",
      "110/110 [==============================] - 51s 468ms/step - loss: 2.4098 - acc: 0.1699 - val_loss: 2.0702 - val_acc: 0.2864\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.25315 to 0.28637, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-05-0.29.h5\n",
      "Epoch 6/200\n",
      "110/110 [==============================] - 52s 470ms/step - loss: 2.3236 - acc: 0.2131 - val_loss: 2.0208 - val_acc: 0.3087\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.28637 to 0.30871, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-06-0.31.h5\n",
      "Epoch 7/200\n",
      "110/110 [==============================] - 52s 471ms/step - loss: 2.2174 - acc: 0.2290 - val_loss: 1.9756 - val_acc: 0.3356\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.30871 to 0.33562, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-07-0.34.h5\n",
      "Epoch 8/200\n",
      "110/110 [==============================] - 51s 466ms/step - loss: 2.2014 - acc: 0.2432 - val_loss: 1.9340 - val_acc: 0.3551\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.33562 to 0.35510, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-08-0.36.h5\n",
      "Epoch 9/200\n",
      "110/110 [==============================] - 52s 471ms/step - loss: 2.1064 - acc: 0.2801 - val_loss: 1.8887 - val_acc: 0.3706\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.35510 to 0.37056, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-09-0.37.h5\n",
      "Epoch 10/200\n",
      "110/110 [==============================] - 51s 467ms/step - loss: 2.0347 - acc: 0.2960 - val_loss: 1.8420 - val_acc: 0.3866\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.37056 to 0.38660, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-10-0.39.h5\n",
      "Epoch 11/200\n",
      "110/110 [==============================] - 52s 469ms/step - loss: 2.0042 - acc: 0.3119 - val_loss: 1.8060 - val_acc: 0.3992\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.38660 to 0.39920, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-11-0.40.h5\n",
      "Epoch 12/200\n",
      "110/110 [==============================] - 53s 478ms/step - loss: 1.9229 - acc: 0.3352 - val_loss: 1.7703 - val_acc: 0.4084\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.39920 to 0.40836, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-12-0.41.h5\n",
      "Epoch 13/200\n",
      "110/110 [==============================] - 54s 487ms/step - loss: 1.8805 - acc: 0.3625 - val_loss: 1.7409 - val_acc: 0.4107\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.40836 to 0.41065, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-13-0.41.h5\n",
      "Epoch 14/200\n",
      "110/110 [==============================] - 55s 496ms/step - loss: 1.7969 - acc: 0.3960 - val_loss: 1.7075 - val_acc: 0.4233\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.41065 to 0.42325, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-14-0.42.h5\n",
      "Epoch 15/200\n",
      "110/110 [==============================] - 55s 502ms/step - loss: 1.7430 - acc: 0.4051 - val_loss: 1.6839 - val_acc: 0.4347\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.42325 to 0.43471, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-15-0.43.h5\n",
      "Epoch 16/200\n",
      "110/110 [==============================] - 53s 482ms/step - loss: 1.7207 - acc: 0.4028 - val_loss: 1.6681 - val_acc: 0.4399\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.43471 to 0.43986, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-16-0.44.h5\n",
      "Epoch 17/200\n",
      "110/110 [==============================] - 53s 478ms/step - loss: 1.6607 - acc: 0.4426 - val_loss: 1.6448 - val_acc: 0.4548\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.43986 to 0.45475, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-17-0.45.h5\n",
      "Epoch 18/200\n",
      "110/110 [==============================] - 52s 472ms/step - loss: 1.6058 - acc: 0.4517 - val_loss: 1.6272 - val_acc: 0.4639\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.45475 to 0.46392, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-18-0.46.h5\n",
      "Epoch 19/200\n",
      "110/110 [==============================] - 51s 467ms/step - loss: 1.5454 - acc: 0.4858 - val_loss: 1.6038 - val_acc: 0.4742\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.46392 to 0.47423, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-19-0.47.h5\n",
      "Epoch 20/200\n",
      "110/110 [==============================] - 51s 465ms/step - loss: 1.5076 - acc: 0.4869 - val_loss: 1.5887 - val_acc: 0.4822\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.47423 to 0.48225, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-20-0.48.h5\n",
      "Epoch 21/200\n",
      "110/110 [==============================] - 51s 465ms/step - loss: 1.4534 - acc: 0.5040 - val_loss: 1.5768 - val_acc: 0.4914\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.48225 to 0.49141, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-21-0.49.h5\n",
      "Epoch 22/200\n",
      "110/110 [==============================] - 51s 464ms/step - loss: 1.3927 - acc: 0.5307 - val_loss: 1.5654 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.49141 to 0.50000, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-22-0.50.h5\n",
      "Epoch 23/200\n",
      "110/110 [==============================] - 51s 465ms/step - loss: 1.3556 - acc: 0.5585 - val_loss: 1.5746 - val_acc: 0.5017\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.50000 to 0.50172, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-23-0.50.h5\n",
      "Epoch 24/200\n",
      "110/110 [==============================] - 51s 459ms/step - loss: 1.2853 - acc: 0.5733 - val_loss: 1.5614 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.50172 to 0.51260, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-24-0.51.h5\n",
      "Epoch 25/200\n",
      "110/110 [==============================] - 51s 462ms/step - loss: 1.2925 - acc: 0.5608 - val_loss: 1.5515 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.51260 to 0.52291, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-25-0.52.h5\n",
      "Epoch 26/200\n",
      "110/110 [==============================] - 51s 460ms/step - loss: 1.2492 - acc: 0.5869 - val_loss: 1.5471 - val_acc: 0.5246\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.52291 to 0.52463, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-26-0.52.h5\n",
      "Epoch 27/200\n",
      "110/110 [==============================] - 51s 461ms/step - loss: 1.1561 - acc: 0.6256 - val_loss: 1.5534 - val_acc: 0.5212\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.52463\n",
      "Epoch 28/200\n",
      "110/110 [==============================] - 50s 457ms/step - loss: 1.1558 - acc: 0.6182 - val_loss: 1.5598 - val_acc: 0.5189\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.52463\n",
      "Epoch 29/200\n",
      "110/110 [==============================] - 51s 462ms/step - loss: 1.0764 - acc: 0.6398 - val_loss: 1.5692 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.52463 to 0.52978, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-29-0.53.h5\n",
      "Epoch 30/200\n",
      "110/110 [==============================] - 51s 463ms/step - loss: 1.0634 - acc: 0.6489 - val_loss: 1.5648 - val_acc: 0.5367\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.52978 to 0.53666, saving model to experiments/RESNET50_1/checkpoints/RESNET50_1-30-0.54.h5\n",
      "Epoch 31/200\n",
      "110/110 [==============================] - 51s 462ms/step - loss: 1.0093 - acc: 0.6739 - val_loss: 1.5843 - val_acc: 0.5361\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.53666\n",
      "Epoch 32/200\n",
      "110/110 [==============================] - 50s 458ms/step - loss: 0.9522 - acc: 0.6943 - val_loss: 1.5918 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.53666\n",
      "Epoch 33/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.8882 - acc: 0.7131 - val_loss: 1.5980 - val_acc: 0.5304\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.53666\n",
      "Epoch 34/200\n",
      "110/110 [==============================] - 50s 458ms/step - loss: 0.8983 - acc: 0.7068 - val_loss: 1.5999 - val_acc: 0.5349\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.53666\n",
      "Epoch 35/200\n",
      "110/110 [==============================] - 51s 465ms/step - loss: 0.8556 - acc: 0.7227 - val_loss: 1.6264 - val_acc: 0.5361\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.53666\n",
      "Epoch 36/200\n",
      "110/110 [==============================] - 50s 457ms/step - loss: 0.7901 - acc: 0.7375 - val_loss: 1.6434 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.53666\n",
      "Epoch 37/200\n",
      "110/110 [==============================] - 50s 455ms/step - loss: 0.7556 - acc: 0.7432 - val_loss: 1.6562 - val_acc: 0.5338\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.53666\n",
      "Epoch 38/200\n",
      "110/110 [==============================] - 50s 458ms/step - loss: 0.7302 - acc: 0.7642 - val_loss: 1.6673 - val_acc: 0.5326\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.53666\n",
      "Epoch 39/200\n",
      "110/110 [==============================] - 51s 463ms/step - loss: 0.6879 - acc: 0.7784 - val_loss: 1.6795 - val_acc: 0.5275\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.53666\n",
      "Epoch 40/200\n",
      "110/110 [==============================] - 51s 462ms/step - loss: 0.6332 - acc: 0.7920 - val_loss: 1.6972 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.53666\n",
      "Epoch 41/200\n",
      "110/110 [==============================] - 52s 472ms/step - loss: 0.6049 - acc: 0.8114 - val_loss: 1.7304 - val_acc: 0.5178\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.53666\n",
      "Epoch 42/200\n",
      "110/110 [==============================] - 52s 475ms/step - loss: 0.5733 - acc: 0.8074 - val_loss: 1.7436 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.53666\n",
      "Epoch 43/200\n",
      "110/110 [==============================] - 52s 474ms/step - loss: 0.5302 - acc: 0.8341 - val_loss: 1.7955 - val_acc: 0.5206\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.53666\n",
      "Epoch 44/200\n",
      "110/110 [==============================] - 51s 465ms/step - loss: 0.5064 - acc: 0.8432 - val_loss: 1.7995 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.53666\n",
      "Epoch 45/200\n",
      "110/110 [==============================] - 50s 458ms/step - loss: 0.4960 - acc: 0.8449 - val_loss: 1.8498 - val_acc: 0.5189\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.53666\n",
      "Epoch 46/200\n",
      "110/110 [==============================] - 51s 461ms/step - loss: 0.4447 - acc: 0.8636 - val_loss: 1.8579 - val_acc: 0.5206\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.53666\n",
      "Epoch 47/200\n",
      "110/110 [==============================] - 51s 465ms/step - loss: 0.4234 - acc: 0.8619 - val_loss: 1.8731 - val_acc: 0.5195\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.53666\n",
      "Epoch 48/200\n",
      "110/110 [==============================] - 51s 459ms/step - loss: 0.3916 - acc: 0.8807 - val_loss: 1.8959 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.53666\n",
      "Epoch 49/200\n",
      "110/110 [==============================] - 51s 460ms/step - loss: 0.3814 - acc: 0.8733 - val_loss: 1.9353 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.53666\n",
      "Epoch 50/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.3743 - acc: 0.8841 - val_loss: 1.9365 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.53666\n",
      "Epoch 51/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.3425 - acc: 0.8949 - val_loss: 1.9437 - val_acc: 0.5223\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.53666\n",
      "Epoch 52/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.3248 - acc: 0.8972 - val_loss: 1.9868 - val_acc: 0.5212\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.53666\n",
      "Epoch 53/200\n",
      "110/110 [==============================] - 50s 459ms/step - loss: 0.3217 - acc: 0.8994 - val_loss: 1.9899 - val_acc: 0.5269\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.53666\n",
      "Epoch 54/200\n",
      "110/110 [==============================] - 50s 455ms/step - loss: 0.2811 - acc: 0.9159 - val_loss: 2.0376 - val_acc: 0.5304\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.53666\n",
      "Epoch 55/200\n",
      "110/110 [==============================] - 50s 454ms/step - loss: 0.2448 - acc: 0.9301 - val_loss: 2.0618 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.53666\n",
      "Epoch 56/200\n",
      "110/110 [==============================] - 50s 452ms/step - loss: 0.2736 - acc: 0.9080 - val_loss: 2.1134 - val_acc: 0.5241\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.53666\n",
      "Epoch 57/200\n",
      "110/110 [==============================] - 50s 453ms/step - loss: 0.2374 - acc: 0.9250 - val_loss: 2.1458 - val_acc: 0.5235\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.53666\n",
      "Epoch 58/200\n",
      "110/110 [==============================] - 50s 451ms/step - loss: 0.2066 - acc: 0.9364 - val_loss: 2.1386 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.53666\n",
      "Epoch 59/200\n",
      "110/110 [==============================] - 50s 458ms/step - loss: 0.2168 - acc: 0.9301 - val_loss: 2.2426 - val_acc: 0.5097\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.53666\n",
      "Epoch 60/200\n",
      "110/110 [==============================] - 50s 457ms/step - loss: 0.2038 - acc: 0.9392 - val_loss: 2.2261 - val_acc: 0.5132\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.53666\n",
      "Epoch 61/200\n",
      "110/110 [==============================] - 50s 453ms/step - loss: 0.1982 - acc: 0.9415 - val_loss: 2.2636 - val_acc: 0.5120\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.53666\n",
      "Epoch 62/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.1885 - acc: 0.9426 - val_loss: 2.2495 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.53666\n",
      "Epoch 63/200\n",
      "110/110 [==============================] - 50s 455ms/step - loss: 0.1759 - acc: 0.9455 - val_loss: 2.2606 - val_acc: 0.5086\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.53666\n",
      "Epoch 64/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.1415 - acc: 0.9591 - val_loss: 2.2662 - val_acc: 0.5166\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.53666\n",
      "Epoch 65/200\n",
      "110/110 [==============================] - 50s 457ms/step - loss: 0.1483 - acc: 0.9568 - val_loss: 2.2787 - val_acc: 0.5103\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.53666\n",
      "Epoch 66/200\n",
      "110/110 [==============================] - 50s 459ms/step - loss: 0.1506 - acc: 0.9557 - val_loss: 2.3166 - val_acc: 0.5092\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.53666\n",
      "Epoch 67/200\n",
      "110/110 [==============================] - 50s 457ms/step - loss: 0.1392 - acc: 0.9597 - val_loss: 2.3577 - val_acc: 0.5120\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.53666\n",
      "Epoch 68/200\n",
      "110/110 [==============================] - 50s 457ms/step - loss: 0.1328 - acc: 0.9602 - val_loss: 2.4076 - val_acc: 0.5172\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.53666\n",
      "Epoch 69/200\n",
      "110/110 [==============================] - 50s 456ms/step - loss: 0.1167 - acc: 0.9682 - val_loss: 2.4242 - val_acc: 0.5172\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.53666\n",
      "Epoch 70/200\n",
      "110/110 [==============================] - 51s 461ms/step - loss: 0.1299 - acc: 0.9602 - val_loss: 2.4262 - val_acc: 0.5143\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.53666\n",
      "Epoch 71/200\n",
      "110/110 [==============================] - 51s 464ms/step - loss: 0.1026 - acc: 0.9756 - val_loss: 2.4188 - val_acc: 0.5160\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.53666\n",
      "Epoch 72/200\n",
      "110/110 [==============================] - 52s 470ms/step - loss: 0.0934 - acc: 0.9710 - val_loss: 2.4425 - val_acc: 0.5235\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.53666\n",
      "Epoch 73/200\n",
      "110/110 [==============================] - 52s 470ms/step - loss: 0.0946 - acc: 0.9739 - val_loss: 2.5050 - val_acc: 0.5195\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.53666\n",
      "Epoch 74/200\n",
      "110/110 [==============================] - 52s 472ms/step - loss: 0.1043 - acc: 0.9705 - val_loss: 2.5169 - val_acc: 0.5074\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.53666\n",
      "Epoch 75/200\n",
      " 43/110 [==========>...................] - ETA: 12s - loss: 0.0663 - acc: 0.9826[INFO]: You have entered CTRL+C.. Wait to finalize\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6Id1K7WHMhN"
   },
   "source": [
    "Testing on best epoch \"RESNET50_1-30-0.54.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bLYsEGT4rjGb",
    "outputId": "001a5169-c4c4-42d7-8540-f0962999c013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 2,\n",
      " 'best_file': 'RESNET50_1-30-0.54.h5',\n",
      " 'callbacks': {'checkpoint_mode': 'max',\n",
      "               'checkpoint_monitor': 'val_acc',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': True,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': True,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_1',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 100,\n",
      " 'mode': 'test',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [128, 128],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_1\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-03 00:46:49.007143: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-03 00:46:49.012460: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000160000 Hz\n",
      "2020-01-03 00:46:49.012685: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e93180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 00:46:49.012715: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-03 00:46:49.014680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-03 00:46:49.127377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 00:46:49.128271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e92fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 00:46:49.128313: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-01-03 00:46:49.128526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 00:46:49.129164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-03 00:46:49.129234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-03 00:46:49.130927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-03 00:46:49.132558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-03 00:46:49.132883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-03 00:46:49.134492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-03 00:46:49.135268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-03 00:46:49.138407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-03 00:46:49.138545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 00:46:49.139176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 00:46:49.139663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-03 00:46:52.018082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-03 00:46:52.018156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-03 00:46:52.018168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-03 00:46:52.018434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 00:46:52.019109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 00:46:52.019609: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-03 00:46:52.019655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "1746/1746 [==============================] - 28s 16ms/step\n",
      "[1.5648373618534157, 0.5366552119129439]\n",
      "1746/1746 [==============================] - 27s 15ms/step\n",
      "[[111  24  15   4   5   4   2   5   3   2   0]\n",
      " [ 24 131  25  27  43   4   8   7   8  16   5]\n",
      " [  6  39  90   8  23   2   6   0   8  10   4]\n",
      " [  2  18   5 101  12  14   1   0   1   4   0]\n",
      " [  4  35   5  10  94   3  13   0   1  22   2]\n",
      " [  8  13   1   6   6  70   4   6   0   0   1]\n",
      " [  2  12   4   3   7   3 110   0  15   0   0]\n",
      " [ 18   7   0   1   5   7   0  29   2  14   6]\n",
      " [  4  34   9   4   9   2   3   4  64   6  14]\n",
      " [  1   9   9   3  16   0   1   0   0 101   0]\n",
      " [  8  23   0   0   0   0   1   5   4   0  36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61       175\n",
      "           1       0.38      0.44      0.41       298\n",
      "           2       0.55      0.46      0.50       196\n",
      "           3       0.60      0.64      0.62       158\n",
      "           4       0.43      0.50      0.46       189\n",
      "           5       0.64      0.61      0.62       115\n",
      "           6       0.74      0.71      0.72       156\n",
      "           7       0.52      0.33      0.40        89\n",
      "           8       0.60      0.42      0.49       153\n",
      "           9       0.58      0.72      0.64       140\n",
      "          10       0.53      0.47      0.50        77\n",
      "\n",
      "    accuracy                           0.54      1746\n",
      "   macro avg       0.56      0.54      0.54      1746\n",
      "weighted avg       0.54      0.54      0.54      1746\n",
      "\n",
      "937\n",
      "0.5366552119129439\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_1_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with cropped images with (512, 512) size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7CCxT3j_HL4x",
    "outputId": "32095ac4-31af-477f-d8aa-80e1425c8fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 4,\n",
      " 'callbacks': {'checkpoint_mode': 'max',\n",
      "               'checkpoint_monitor': 'val_acc',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': False,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': True,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_2',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 200,\n",
      " 'mode': 'train',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [512, 512],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_2\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-03 11:33:32.687981: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-03 11:33:32.692517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz\n",
      "2020-01-03 11:33:32.692754: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ae7180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 11:33:32.692784: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-03 11:33:32.694623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-03 11:33:32.812825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 11:33:32.813332: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ae6fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 11:33:32.813358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
      "2020-01-03 11:33:32.813535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 11:33:32.813923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-03 11:33:32.813983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-03 11:33:32.815481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-03 11:33:32.817017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-03 11:33:32.817369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-03 11:33:32.818842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-03 11:33:32.819526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-03 11:33:32.822461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-03 11:33:32.822612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 11:33:32.823013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 11:33:32.823329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-03 11:33:35.768383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-03 11:33:35.768449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-03 11:33:35.768460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-03 11:33:35.768740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 11:33:35.769200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 11:33:35.769561: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-03 11:33:35.769620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6726 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 256, 256, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 256, 256, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 128, 128, 64) 4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 128, 128, 64) 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 128, 128, 64) 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 128, 128, 256 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 128, 128, 256 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 128, 128, 256 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 128, 128, 64) 16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 128, 128, 64) 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 128, 128, 64) 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 128, 128, 256 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 128, 128, 256 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 128, 128, 64) 16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 128, 128, 64) 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 128, 128, 64) 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 128, 128, 256 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 128, 128, 256 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 64, 64, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 64, 64, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 64, 64, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 64, 64, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 64, 64, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 64, 64, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 64, 64, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 64, 64, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 64, 64, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 64, 64, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 64, 64, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 64, 64, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 64, 64, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 64, 64, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 32, 32, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 32, 32, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 32, 32, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 32, 32, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 32, 32, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 32, 32, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 32, 32, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 32, 32, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 32, 32, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 32, 32, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 32, 32, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 32, 32, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 32, 32, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 32, 32, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 32, 32, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 32, 32, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 32, 32, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 32, 32, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 32, 32, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 32, 32, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 32, 32, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 32, 32, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 32, 32, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 32, 32, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 32, 32, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 32, 32, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 16, 16, 512)  524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 16, 16, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 16, 16, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 16, 16, 2048) 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 16, 16, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 16, 16, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 16, 16, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 16, 16, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 16, 16, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 16, 16, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 16, 16, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 16, 16, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 16, 16, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 16, 16, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 16, 16, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 16, 16, 2048) 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/200\n",
      " 64/439 [===>..........................] - ETA: 8:07 - loss: 2.7317 - acc: 0.1250/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:709: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n",
      "439/439 [==============================] - 1343s 3s/step - loss: 2.5231 - acc: 0.1321 - val_loss: 2.2066 - val_acc: 0.2085\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.20848, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-01-0.21.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/200\n",
      "439/439 [==============================] - 241s 550ms/step - loss: 2.3015 - acc: 0.1902 - val_loss: 2.0759 - val_acc: 0.3207\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.20848 to 0.32073, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-02-0.32.h5\n",
      "Epoch 3/200\n",
      "439/439 [==============================] - 241s 549ms/step - loss: 2.2210 - acc: 0.1953 - val_loss: 1.9570 - val_acc: 0.3671\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.32073 to 0.36712, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-03-0.37.h5\n",
      "Epoch 4/200\n",
      "439/439 [==============================] - 241s 550ms/step - loss: 2.0774 - acc: 0.2677 - val_loss: 1.8071 - val_acc: 0.4330\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.36712 to 0.43299, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-04-0.43.h5\n",
      "Epoch 5/200\n",
      "439/439 [==============================] - 240s 548ms/step - loss: 1.9912 - acc: 0.3092 - val_loss: 1.6907 - val_acc: 0.4593\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.43299 to 0.45934, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-05-0.46.h5\n",
      "Epoch 6/200\n",
      "439/439 [==============================] - 239s 545ms/step - loss: 1.8760 - acc: 0.3633 - val_loss: 1.5819 - val_acc: 0.5212\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.45934 to 0.52119, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-06-0.52.h5\n",
      "Epoch 7/200\n",
      "439/439 [==============================] - 240s 548ms/step - loss: 1.7518 - acc: 0.4015 - val_loss: 1.5438 - val_acc: 0.5315\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.52119 to 0.53150, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-07-0.53.h5\n",
      "Epoch 8/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 1.6650 - acc: 0.4527 - val_loss: 1.5376 - val_acc: 0.5275\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.53150\n",
      "Epoch 9/200\n",
      "439/439 [==============================] - 241s 548ms/step - loss: 1.5662 - acc: 0.4772 - val_loss: 1.4480 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.53150 to 0.55556, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-09-0.56.h5\n",
      "Epoch 10/200\n",
      "439/439 [==============================] - 240s 548ms/step - loss: 1.4591 - acc: 0.5302 - val_loss: 1.4399 - val_acc: 0.5819\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.55556 to 0.58190, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-10-0.58.h5\n",
      "Epoch 11/200\n",
      "439/439 [==============================] - 241s 548ms/step - loss: 1.3631 - acc: 0.5661 - val_loss: 1.4665 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.58190\n",
      "Epoch 12/200\n",
      "439/439 [==============================] - 241s 549ms/step - loss: 1.2574 - acc: 0.6156 - val_loss: 1.4203 - val_acc: 0.5842\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.58190 to 0.58419, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-12-0.58.h5\n",
      "Epoch 13/200\n",
      "439/439 [==============================] - 241s 548ms/step - loss: 1.1375 - acc: 0.6464 - val_loss: 1.4967 - val_acc: 0.5956\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.58419 to 0.59565, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-13-0.60.h5\n",
      "Epoch 14/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 1.0722 - acc: 0.6686 - val_loss: 1.5332 - val_acc: 0.5939\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.59565\n",
      "Epoch 15/200\n",
      "439/439 [==============================] - 239s 544ms/step - loss: 0.9637 - acc: 0.6942 - val_loss: 1.4674 - val_acc: 0.5997\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.59565 to 0.59966, saving model to experiments/RESNET50_2/checkpoints/RESNET50_2-15-0.60.h5\n",
      "Epoch 16/200\n",
      "439/439 [==============================] - 239s 545ms/step - loss: 0.8853 - acc: 0.7272 - val_loss: 1.6108 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.59966\n",
      "Epoch 17/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 0.7955 - acc: 0.7563 - val_loss: 1.5492 - val_acc: 0.5928\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.59966\n",
      "Epoch 18/200\n",
      "439/439 [==============================] - 240s 548ms/step - loss: 0.7428 - acc: 0.7745 - val_loss: 1.5304 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.59966\n",
      "Epoch 19/200\n",
      "439/439 [==============================] - 241s 549ms/step - loss: 0.6710 - acc: 0.7961 - val_loss: 1.5876 - val_acc: 0.5962\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.59966\n",
      "Epoch 20/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 0.6026 - acc: 0.8166 - val_loss: 1.6597 - val_acc: 0.5859\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.59966\n",
      "Epoch 21/200\n",
      "439/439 [==============================] - 240s 548ms/step - loss: 0.5622 - acc: 0.8257 - val_loss: 1.6701 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.59966\n",
      "Epoch 22/200\n",
      "439/439 [==============================] - 240s 547ms/step - loss: 0.4708 - acc: 0.8519 - val_loss: 1.6910 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.59966\n",
      "Epoch 23/200\n",
      "439/439 [==============================] - 238s 543ms/step - loss: 0.4291 - acc: 0.8713 - val_loss: 1.8567 - val_acc: 0.5848\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.59966\n",
      "Epoch 24/200\n",
      "439/439 [==============================] - 239s 544ms/step - loss: 0.3973 - acc: 0.8736 - val_loss: 1.7591 - val_acc: 0.5916\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.59966\n",
      "Epoch 25/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 0.3530 - acc: 0.8946 - val_loss: 1.7862 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.59966\n",
      "Epoch 26/200\n",
      "439/439 [==============================] - 239s 544ms/step - loss: 0.3432 - acc: 0.8969 - val_loss: 1.8733 - val_acc: 0.5624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.59966\n",
      "Epoch 27/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 0.3225 - acc: 0.9032 - val_loss: 1.8882 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.59966\n",
      "Epoch 28/200\n",
      "439/439 [==============================] - 239s 544ms/step - loss: 0.2710 - acc: 0.9203 - val_loss: 1.9216 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.59966\n",
      "Epoch 29/200\n",
      "439/439 [==============================] - 238s 543ms/step - loss: 0.2445 - acc: 0.9243 - val_loss: 1.8260 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.59966\n",
      "Epoch 30/200\n",
      "439/439 [==============================] - 240s 547ms/step - loss: 0.2184 - acc: 0.9368 - val_loss: 2.0298 - val_acc: 0.5934\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.59966\n",
      "Epoch 31/200\n",
      "439/439 [==============================] - 240s 546ms/step - loss: 0.1934 - acc: 0.9482 - val_loss: 2.0644 - val_acc: 0.5888\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.59966\n",
      "Epoch 32/200\n",
      "439/439 [==============================] - 238s 543ms/step - loss: 0.1913 - acc: 0.9442 - val_loss: 2.0696 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.59966\n",
      "Epoch 33/200\n",
      "438/439 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9498[INFO]: You have entered CTRL+C.. Wait to finalize\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWUcRv3gxr0f"
   },
   "source": [
    "Testing with best epoch \"RESNET50_2-13-0.60.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K64XP4Naxqkb",
    "outputId": "13a9503b-0eee-4e25-bbee-9f9e21b123a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 4,\n",
      " 'best_file': 'RESNET50_2-13-0.60.h5',\n",
      " 'callbacks': {'checkpoint_mode': 'max',\n",
      "               'checkpoint_monitor': 'val_acc',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': False,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': True,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'RESNET50_2',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 200,\n",
      " 'mode': 'test',\n",
      " 'model': 'ResNet50',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [512, 512],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is RESNET50_2\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-05 22:18:38.258848: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-05 22:18:38.263235: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000179999 Hz\n",
      "2020-01-05 22:18:38.263457: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3edb800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-05 22:18:38.263484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-05 22:18:38.265270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-05 22:18:38.365728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 22:18:38.366465: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3edb640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-05 22:18:38.366494: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2020-01-05 22:18:38.366648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 22:18:38.367130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-05 22:18:38.367198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-05 22:18:38.368999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-05 22:18:38.370475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-05 22:18:38.370785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-05 22:18:38.372132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-05 22:18:38.372868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-05 22:18:38.375967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-05 22:18:38.376077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 22:18:38.376688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 22:18:38.377161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-05 22:18:41.303762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-05 22:18:41.303826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-05 22:18:41.303839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-05 22:18:41.304115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 22:18:41.304855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-05 22:18:41.305383: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-05 22:18:41.305418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 256, 256, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 256, 256, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 128, 128, 64) 4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 128, 128, 64) 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 128, 128, 64) 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 128, 128, 256 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 128, 128, 256 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 128, 128, 256 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 128, 128, 64) 16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 128, 128, 64) 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 128, 128, 64) 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 128, 128, 256 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 128, 128, 256 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 128, 128, 64) 16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 128, 128, 64) 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 128, 128, 64) 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 128, 128, 256 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 128, 128, 256 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 64, 64, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 64, 64, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 64, 64, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 64, 64, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 64, 64, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 64, 64, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 64, 64, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 64, 64, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 64, 64, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 64, 64, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 64, 64, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 64, 64, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 64, 64, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 64, 64, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 32, 32, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 32, 32, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 32, 32, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 32, 32, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 32, 32, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 32, 32, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 32, 32, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 32, 32, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 32, 32, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 32, 32, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 32, 32, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 32, 32, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 32, 32, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 32, 32, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 32, 32, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 32, 32, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 32, 32, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 32, 32, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 32, 32, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 32, 32, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 32, 32, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 32, 32, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 32, 32, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 32, 32, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 32, 32, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 32, 32, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 16, 16, 512)  524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 16, 16, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 16, 16, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 16, 16, 2048) 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 16, 16, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 16, 16, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 16, 16, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 16, 16, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 16, 16, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 16, 16, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 16, 16, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 16, 16, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 16, 16, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 16, 16, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 16, 16, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 16, 16, 2048) 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 11)           5643        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,905,099\n",
      "Trainable params: 24,851,979\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "1746/1746 [==============================] - 40s 23ms/step\n",
      "[1.4967287898541086, 0.5956471935853379]\n",
      "1746/1746 [==============================] - 38s 22ms/step\n",
      "[[114  15  19   2   5   2   1   1   6   6   4]\n",
      " [ 18 156  18  23  31   5   5   3   4  23  12]\n",
      " [  2  22 123   2  27   1   1   0   8   6   4]\n",
      " [  0  17   4 111   5   8   5   0   1   6   1]\n",
      " [  3  26   7   6 108   0  12   2   0  22   3]\n",
      " [  5   5   0   6   5  67  14   7   1   2   3]\n",
      " [  1  12   3   3   5   4 100   0  23   1   4]\n",
      " [ 17   9   2   0   3   9   1  19   2  18   9]\n",
      " [  6  15  15   5   1   2   6   0  90   2  11]\n",
      " [  0  10   3   3   5   0   0   0   0 117   2]\n",
      " [  6  26   1   1   0   4   1   0   3   0  35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.66       175\n",
      "           1       0.50      0.52      0.51       298\n",
      "           2       0.63      0.63      0.63       196\n",
      "           3       0.69      0.70      0.69       158\n",
      "           4       0.55      0.57      0.56       189\n",
      "           5       0.66      0.58      0.62       115\n",
      "           6       0.68      0.64      0.66       156\n",
      "           7       0.59      0.21      0.31        89\n",
      "           8       0.65      0.59      0.62       153\n",
      "           9       0.58      0.84      0.68       140\n",
      "          10       0.40      0.45      0.42        77\n",
      "\n",
      "    accuracy                           0.60      1746\n",
      "   macro avg       0.60      0.58      0.58      1746\n",
      "weighted avg       0.60      0.60      0.59      1746\n",
      "\n",
      "1040\n",
      "0.5956471935853379\n"
     ]
    }
   ],
   "source": [
    "!python main.py configs/RESNET50_2_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Smaller model VGG with cropping (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZQ9ZSj6UUSaU",
    "outputId": "1d9fb45f-bbee-4c29-8bcc-cea7702d671f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      " THE Configuration of your experiment ..\n",
      "{'agent': 'VOCACTIVITY_Agent',\n",
      " 'batch_size': 16,\n",
      " 'callbacks': {'checkpoint_mode': 'max',\n",
      "               'checkpoint_monitor': 'val_acc',\n",
      "               'checkpoint_save_best_only': True,\n",
      "               'checkpoint_save_weights_only': True,\n",
      "               'checkpoint_verbose': True,\n",
      "               'tensorboard_write_graph': True},\n",
      " 'classes_num': 11,\n",
      " 'crop_images': True,\n",
      " 'data_loader': 'VOCDataLoader',\n",
      " 'exp_name': 'VGG16_0',\n",
      " 'img_root': 'data/images/',\n",
      " 'learning_rate': 1e-05,\n",
      " 'max_epoch': 150,\n",
      " 'mode': 'train',\n",
      " 'model': 'VGG16',\n",
      " 'num_classes': 11,\n",
      " 'num_workers': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'person_num': 1,\n",
      " 'resize': [512, 512],\n",
      " 'seed': 1337,\n",
      " 'single_label': True,\n",
      " 'train_pickle': 'data/train_data.pickle',\n",
      " 'use_scheduler': False,\n",
      " 'valid_pickle': 'data/valid_data.pickle',\n",
      " 'verbose_training': 1}\n",
      " *************************************** \n",
      "The experiment name is VGG16_0\n",
      " *************************************** \n",
      "[INFO]: Hi, This is root.\n",
      "[INFO]: After the configurations are successfully processed and dirs are created.\n",
      "[INFO]: The pipeline of the project will begin now.\n",
      "[INFO]: NumExpr defaulting to 2 threads.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-01-03 15:57:33.044469: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2020-01-03 15:57:33.049153: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz\n",
      "2020-01-03 15:57:33.049353: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ee6bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 15:57:33.049380: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-01-03 15:57:33.051138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-01-03 15:57:33.176038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 15:57:33.176624: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ee6d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-01-03 15:57:33.176654: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
      "2020-01-03 15:57:33.176819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 15:57:33.177153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-01-03 15:57:33.177217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-01-03 15:57:33.178721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-01-03 15:57:33.180263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-01-03 15:57:33.180595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-01-03 15:57:33.181967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-01-03 15:57:33.182772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-01-03 15:57:33.185540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-01-03 15:57:33.185668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 15:57:33.186052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 15:57:33.186372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-01-03 15:57:36.011183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-01-03 15:57:36.011240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-01-03 15:57:36.011252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-01-03 15:57:36.011537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 15:57:36.012012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-01-03 15:57:36.012341: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-01-03 15:57:36.012383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6726 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 512, 512, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 256, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 128, 128, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 64, 64, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 11)                5643      \n",
      "=================================================================\n",
      "Total params: 15,245,643\n",
      "Trainable params: 15,245,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/150\n",
      "2020-01-03 15:57:41.953487: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:42.473733: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:44.350319: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:45.509682: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.15GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:46.381298: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:46.799130: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:47.022362: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.70GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2020-01-03 15:57:47.269489: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.70GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      " 88/110 [=======================>......] - ETA: 34s - loss: 2.6241 - acc: 0.10012020-01-03 15:59:58.788229: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "110/110 [==============================] - 241s 2s/step - loss: 2.5737 - acc: 0.1091 - val_loss: 2.3544 - val_acc: 0.1649\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.16495, saving model to experiments/VGG16_0/checkpoints/VGG16_0-01-0.16.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "[WARNING]: From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - 228s 2s/step - loss: 2.3583 - acc: 0.1494 - val_loss: 2.3106 - val_acc: 0.1838\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.16495 to 0.18385, saving model to experiments/VGG16_0/checkpoints/VGG16_0-02-0.18.h5\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - 228s 2s/step - loss: 2.3445 - acc: 0.1466 - val_loss: 2.3009 - val_acc: 0.1856\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.18385 to 0.18557, saving model to experiments/VGG16_0/checkpoints/VGG16_0-03-0.19.h5\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.3275 - acc: 0.1477 - val_loss: 2.2599 - val_acc: 0.2096\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.18557 to 0.20962, saving model to experiments/VGG16_0/checkpoints/VGG16_0-04-0.21.h5\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 2.3168 - acc: 0.1642 - val_loss: 2.2242 - val_acc: 0.1959\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.20962\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 2.2923 - acc: 0.1642 - val_loss: 2.2141 - val_acc: 0.2199\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.20962 to 0.21993, saving model to experiments/VGG16_0/checkpoints/VGG16_0-06-0.22.h5\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.2565 - acc: 0.1983 - val_loss: 2.2126 - val_acc: 0.2468\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.21993 to 0.24685, saving model to experiments/VGG16_0/checkpoints/VGG16_0-07-0.25.h5\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.2271 - acc: 0.1977 - val_loss: 2.1714 - val_acc: 0.2108\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.24685\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.2014 - acc: 0.2193 - val_loss: 2.0882 - val_acc: 0.2646\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.24685 to 0.26460, saving model to experiments/VGG16_0/checkpoints/VGG16_0-09-0.26.h5\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.1713 - acc: 0.2159 - val_loss: 2.1181 - val_acc: 0.2743\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.26460 to 0.27434, saving model to experiments/VGG16_0/checkpoints/VGG16_0-10-0.27.h5\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.1203 - acc: 0.2449 - val_loss: 2.0414 - val_acc: 0.2921\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.27434 to 0.29210, saving model to experiments/VGG16_0/checkpoints/VGG16_0-11-0.29.h5\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.0709 - acc: 0.2705 - val_loss: 2.0000 - val_acc: 0.2841\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.29210\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 2.0325 - acc: 0.2852 - val_loss: 1.9604 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.29210 to 0.30355, saving model to experiments/VGG16_0/checkpoints/VGG16_0-13-0.30.h5\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.9947 - acc: 0.3057 - val_loss: 1.9631 - val_acc: 0.3436\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.30355 to 0.34364, saving model to experiments/VGG16_0/checkpoints/VGG16_0-14-0.34.h5\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.9617 - acc: 0.2960 - val_loss: 1.9155 - val_acc: 0.3471\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.34364 to 0.34708, saving model to experiments/VGG16_0/checkpoints/VGG16_0-15-0.35.h5\n",
      "Epoch 16/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.9022 - acc: 0.3261 - val_loss: 1.9728 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.34708\n",
      "Epoch 17/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.8699 - acc: 0.3466 - val_loss: 1.8708 - val_acc: 0.3477\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.34708 to 0.34765, saving model to experiments/VGG16_0/checkpoints/VGG16_0-17-0.35.h5\n",
      "Epoch 18/150\n",
      " 30/110 [=======>......................] - ETA: 1:57 - loss: 1.7827 - acc: 0.37292020-01-03 17:03:33.948170: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.7956 - acc: 0.3665 - val_loss: 1.9072 - val_acc: 0.3654\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.34765 to 0.36541, saving model to experiments/VGG16_0/checkpoints/VGG16_0-18-0.37.h5\n",
      "Epoch 19/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.7607 - acc: 0.3909 - val_loss: 1.8558 - val_acc: 0.3683\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.36541 to 0.36827, saving model to experiments/VGG16_0/checkpoints/VGG16_0-19-0.37.h5\n",
      "Epoch 20/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.6990 - acc: 0.4142 - val_loss: 1.7932 - val_acc: 0.4055\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.36827 to 0.40550, saving model to experiments/VGG16_0/checkpoints/VGG16_0-20-0.41.h5\n",
      "Epoch 21/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 1.6420 - acc: 0.4392 - val_loss: 1.8007 - val_acc: 0.4009\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.40550\n",
      "Epoch 22/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.6082 - acc: 0.4318 - val_loss: 1.8634 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.40550\n",
      "Epoch 23/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 1.5762 - acc: 0.4580 - val_loss: 1.8999 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.40550\n",
      "Epoch 24/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.5307 - acc: 0.4744 - val_loss: 1.8337 - val_acc: 0.3906\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.40550\n",
      "Epoch 25/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.5060 - acc: 0.4824 - val_loss: 1.7684 - val_acc: 0.4410\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.40550 to 0.44101, saving model to experiments/VGG16_0/checkpoints/VGG16_0-25-0.44.h5\n",
      "Epoch 26/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.4640 - acc: 0.5097 - val_loss: 1.7327 - val_acc: 0.4410\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.44101\n",
      "Epoch 27/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.3527 - acc: 0.5369 - val_loss: 1.7793 - val_acc: 0.4341\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.44101\n",
      "Epoch 28/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 1.3233 - acc: 0.5545 - val_loss: 1.7777 - val_acc: 0.4410\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.44101\n",
      "Epoch 29/150\n",
      "110/110 [==============================] - 235s 2s/step - loss: 1.2825 - acc: 0.5528 - val_loss: 1.7396 - val_acc: 0.4473\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.44101 to 0.44731, saving model to experiments/VGG16_0/checkpoints/VGG16_0-29-0.45.h5\n",
      "Epoch 30/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.2803 - acc: 0.5540 - val_loss: 1.7678 - val_acc: 0.4422\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.44731\n",
      "Epoch 31/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 1.1643 - acc: 0.6011 - val_loss: 1.9448 - val_acc: 0.4278\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.44731\n",
      "Epoch 32/150\n",
      "110/110 [==============================] - 235s 2s/step - loss: 1.1288 - acc: 0.6148 - val_loss: 1.8638 - val_acc: 0.4399\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.44731\n",
      "Epoch 33/150\n",
      "110/110 [==============================] - 240s 2s/step - loss: 1.0935 - acc: 0.6290 - val_loss: 1.8636 - val_acc: 0.4341\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.44731\n",
      "Epoch 34/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 1.0364 - acc: 0.6347 - val_loss: 1.8273 - val_acc: 0.4301\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.44731\n",
      "Epoch 35/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 1.0118 - acc: 0.6540 - val_loss: 1.8476 - val_acc: 0.4513\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.44731 to 0.45132, saving model to experiments/VGG16_0/checkpoints/VGG16_0-35-0.45.h5\n",
      "Epoch 36/150\n",
      "110/110 [==============================] - 233s 2s/step - loss: 0.9926 - acc: 0.6619 - val_loss: 1.8738 - val_acc: 0.4227\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.45132\n",
      "Epoch 37/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.9268 - acc: 0.6852 - val_loss: 1.8532 - val_acc: 0.4662\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.45132 to 0.46621, saving model to experiments/VGG16_0/checkpoints/VGG16_0-37-0.47.h5\n",
      "Epoch 38/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 0.8908 - acc: 0.6966 - val_loss: 2.0316 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.46621\n",
      "Epoch 39/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.8389 - acc: 0.7125 - val_loss: 1.9331 - val_acc: 0.4605\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.46621\n",
      "Epoch 40/150\n",
      "110/110 [==============================] - 230s 2s/step - loss: 0.8151 - acc: 0.7250 - val_loss: 1.9318 - val_acc: 0.4393\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.46621\n",
      "Epoch 41/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.8016 - acc: 0.7176 - val_loss: 2.0325 - val_acc: 0.4450\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.46621\n",
      "Epoch 42/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.7099 - acc: 0.7545 - val_loss: 2.0000 - val_acc: 0.4519\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.46621\n",
      "Epoch 43/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.6881 - acc: 0.7591 - val_loss: 1.9943 - val_acc: 0.4530\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.46621\n",
      "Epoch 44/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.6373 - acc: 0.7909 - val_loss: 2.0958 - val_acc: 0.4599\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.46621\n",
      "Epoch 45/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.6344 - acc: 0.7881 - val_loss: 2.3478 - val_acc: 0.4427\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.46621\n",
      "Epoch 46/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.5930 - acc: 0.7920 - val_loss: 2.2273 - val_acc: 0.4616\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.46621\n",
      "Epoch 47/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.5690 - acc: 0.8074 - val_loss: 2.3279 - val_acc: 0.4507\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.46621\n",
      "Epoch 48/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.5505 - acc: 0.8148 - val_loss: 2.2326 - val_acc: 0.4427\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.46621\n",
      "Epoch 49/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.5302 - acc: 0.8210 - val_loss: 2.1962 - val_acc: 0.4530\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.46621\n",
      "Epoch 50/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.5004 - acc: 0.8358 - val_loss: 2.2264 - val_acc: 0.4542\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.46621\n",
      "Epoch 51/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.4940 - acc: 0.8364 - val_loss: 2.4702 - val_acc: 0.4622\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.46621\n",
      "Epoch 52/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.4691 - acc: 0.8318 - val_loss: 2.3175 - val_acc: 0.4639\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.46621\n",
      "Epoch 53/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.4173 - acc: 0.8506 - val_loss: 2.4418 - val_acc: 0.4399\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.46621\n",
      "Epoch 54/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.4017 - acc: 0.8580 - val_loss: 2.4532 - val_acc: 0.4507\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.46621\n",
      "Epoch 55/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.3909 - acc: 0.8670 - val_loss: 2.3644 - val_acc: 0.4427\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.46621\n",
      "Epoch 56/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.4448 - acc: 0.8449 - val_loss: 2.4843 - val_acc: 0.4691\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.46621 to 0.46907, saving model to experiments/VGG16_0/checkpoints/VGG16_0-56-0.47.h5\n",
      "Epoch 57/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.3541 - acc: 0.8807 - val_loss: 2.6224 - val_acc: 0.4553\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.46907\n",
      "Epoch 58/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.3514 - acc: 0.8875 - val_loss: 2.5129 - val_acc: 0.4422\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.46907\n",
      "Epoch 59/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.3332 - acc: 0.8886 - val_loss: 2.8063 - val_acc: 0.4490\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.46907\n",
      "Epoch 60/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.3030 - acc: 0.8994 - val_loss: 2.6117 - val_acc: 0.4450\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.46907\n",
      "Epoch 61/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2863 - acc: 0.9091 - val_loss: 2.5727 - val_acc: 0.4622\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.46907\n",
      "Epoch 62/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2703 - acc: 0.9062 - val_loss: 2.5123 - val_acc: 0.4570\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.46907\n",
      "Epoch 63/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2660 - acc: 0.9159 - val_loss: 2.7317 - val_acc: 0.4519\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.46907\n",
      "Epoch 64/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2790 - acc: 0.9045 - val_loss: 2.8634 - val_acc: 0.4525\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.46907\n",
      "Epoch 65/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2806 - acc: 0.9023 - val_loss: 2.9686 - val_acc: 0.4725\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.46907 to 0.47251, saving model to experiments/VGG16_0/checkpoints/VGG16_0-65-0.47.h5\n",
      "Epoch 66/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2538 - acc: 0.9125 - val_loss: 2.7261 - val_acc: 0.4530\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.47251\n",
      "Epoch 67/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2382 - acc: 0.9261 - val_loss: 2.6236 - val_acc: 0.4525\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.47251\n",
      "Epoch 68/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2266 - acc: 0.9267 - val_loss: 2.7956 - val_acc: 0.4674\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.47251\n",
      "Epoch 69/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1992 - acc: 0.9335 - val_loss: 2.9649 - val_acc: 0.4462\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.47251\n",
      "Epoch 70/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1623 - acc: 0.9528 - val_loss: 3.4968 - val_acc: 0.4347\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.47251\n",
      "Epoch 71/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1715 - acc: 0.9398 - val_loss: 2.9945 - val_acc: 0.4616\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.47251\n",
      "Epoch 72/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2008 - acc: 0.9381 - val_loss: 3.2214 - val_acc: 0.4651\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.47251\n",
      "Epoch 73/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1774 - acc: 0.9472 - val_loss: 2.9515 - val_acc: 0.4565\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.47251\n",
      "Epoch 74/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1626 - acc: 0.9477 - val_loss: 3.0646 - val_acc: 0.4456\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.47251\n",
      "Epoch 75/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2108 - acc: 0.9324 - val_loss: 2.9174 - val_acc: 0.4662\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.47251\n",
      "Epoch 76/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1704 - acc: 0.9523 - val_loss: 2.9521 - val_acc: 0.4341\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.47251\n",
      "Epoch 77/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1329 - acc: 0.9580 - val_loss: 3.3161 - val_acc: 0.4496\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.47251\n",
      "Epoch 78/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1482 - acc: 0.9517 - val_loss: 3.4816 - val_acc: 0.4645\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.47251\n",
      "Epoch 79/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1657 - acc: 0.9545 - val_loss: 3.3752 - val_acc: 0.4582\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.47251\n",
      "Epoch 80/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.2018 - acc: 0.9426 - val_loss: 2.9829 - val_acc: 0.4485\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.47251\n",
      "Epoch 81/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1275 - acc: 0.9562 - val_loss: 3.5254 - val_acc: 0.4336\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.47251\n",
      "Epoch 82/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.1182 - acc: 0.9625 - val_loss: 3.2218 - val_acc: 0.4548\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.47251\n",
      "Epoch 83/150\n",
      "110/110 [==============================] - 229s 2s/step - loss: 0.0923 - acc: 0.9733 - val_loss: 3.2229 - val_acc: 0.4416\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.47251\n",
      "Epoch 84/150\n",
      " 52/110 [=============>................] - ETA: 1:25 - loss: 0.1433 - acc: 0.9675"
     ]
    }
   ],
   "source": [
    "!python main.py configs/VGG16_0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfA9OVwlJ4sL"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6jk-PyjUoGz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VOC_Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
